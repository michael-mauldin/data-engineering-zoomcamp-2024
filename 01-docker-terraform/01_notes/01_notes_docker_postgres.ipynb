{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 - Docker + Postres\n",
    "## Introduction to Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker is a set of platform as a service products that use OS-level virtualization to deliver software in packages called **containers**. Containers are isolated from each other and bundle their own software, libraries, and configuration files, though they can communicate with each other through defined channels.\n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "  A(\"Source(csv file)\") --> B[Data Pipeline]\n",
    "  B --> C(\"Destination (database)\")\n",
    "\n",
    "  style A fill: snow, stroke: silver\n",
    "  style B fill: lightskyblue, stroke: lightslategrey\n",
    "  style C fill: snow, stroke: silver\n",
    "```\n",
    "\n",
    "This is useful in data engineering for:\n",
    "\n",
    "- Reproducibility - creating pipelines to be reused across local and cloud environments.\n",
    "- Local experiments - easier to setup local experiments.\n",
    "- Integration test (CI/CD) - allows for smoother integration tests.\n",
    "- Serverless - easier to setup on serverless environments like AWS Lambda, Google Functions, etc.\n",
    "- Spark - easier to setup dependencies for Spark.\n",
    "\n",
    "### Installing Docker\n",
    "\n",
    "Using `apt` from the [Docker docs](https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository):\n",
    "\n",
    "1. Set up Docker's `apt` repository.\n",
    "```default\n",
    "# Add Docker's official GPG key:\n",
    "sudo apt-get update\n",
    "sudo apt-get install ca-certificates curl gnupg\n",
    "sudo install -m 0755 -d /etc/apt/keyrings\n",
    "curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n",
    "sudo chmod a+r /etc/apt/keyrings/docker.gpg\n",
    "\n",
    "# Add the repository to Apt sources:\n",
    "echo \\\n",
    "  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n",
    "  $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\\n",
    "  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n",
    "sudo apt-get update\n",
    "```\n",
    "\n",
    "2. Install the latest Docker packages.\n",
    "```default\n",
    "sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n",
    "```\n",
    "\n",
    "3. Verify that Docker is installed by running the `hello-world` image.\n",
    "```default\n",
    "sudo docker run hello-world\n",
    "```\n",
    "\n",
    ":::{.callout-important}\n",
    "If the Docker daemon isn't running, restart with\n",
    "```default\n",
    "sudo service docker start\n",
    "```\n",
    ":::\n",
    "\n",
    "### Docker Images\n",
    "A Docker image is a read-only template containing a set of instructions for creating a container. Pre-defined images are availabe to download from container registries such as [Docker Hub](https://hub.docker.com).\n",
    "\n",
    "To run a docker image:\n",
    "```default\n",
    "docker run -it ubuntu bash\n",
    "```\n",
    "\n",
    "- `-it` : is used to run the container in interactive mode.\n",
    "- `ubuntu` : the name of the image (docker will first search for the image on Docker Hub).\n",
    "- `bash` : a parameter command used to open bash.\n",
    "\n",
    "To run a Python image:\n",
    "```default\n",
    "docker run -it --entrypoint=bash python:3.9\n",
    "```\n",
    "\n",
    "- `--entrypoint=bash` : control where to enter the container, either in bash or a Python REPL.\n",
    "- `:3.9` : everything after the `:` is a tag, in this case it indicates which version of the Python image to download and use.\n",
    "\n",
    "Docker containers are **stateless**, meaning changes are not persisted between runs.\n",
    "\n",
    "### Dockerfile\n",
    "A Dockerfile is used to define the instructions used to create an image.\n",
    "\n",
    "- Usually begins with `FROM <image name>:<image tag>` to base the new image on a template image.\n",
    "- Use `RUN <command>` to execute commands.\n",
    "- Use `ENTRYPOINT <command>` to define how to enter the container.\n",
    "- Use `WORKDIR <path>` to define the container's working directory.\n",
    "- Use `COPY <source> <destination>` to copy files from the local working directory to the container working directory.\n",
    "\n",
    "``` {.Dockerfile}\n",
    "FROM python:3.11\n",
    "\n",
    "RUN apt-get install wget\n",
    "RUN pip install pandas sqlalchemy psycopg2\n",
    "\n",
    "WORKDIR /app\n",
    "COPY ingest_data.py ingest_data.py\n",
    "\n",
    "ENTRYPOINT [\"bash\"] # or [\"python\", \"pipeline.py\"]\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Building the image\n",
    "Once the Dockerfile is complete, it is used to build the image with `docker build`.\n",
    "```default\n",
    "docker build -t test:pandas .\n",
    "```\n",
    "\n",
    "- `-t` : is used to set an image name and tag.\n",
    "- `test` : the name of the image.\n",
    "- `pandas` : is the tag (usually used for versioning).\n",
    "- `.` : the path to the Dockerfile, `.` indicates the current directory.\n",
    "\n",
    "\n",
    "### Running Postgres in Docker\n",
    "Postgres is a popular sql database management system. Running Postgres in docker requires the use of environmental variables, volumes, and ports. **Volumes** map a local host filesystem with a container filesytem to persist data. To communicate with our database inside the container we will need to map a **port** on our local host machine to a port in the container. To run the Postgres image:\n",
    "\n",
    "```default\n",
    "docker run -it \\\n",
    "    -e POSTGRES_USER=\"root\" \\\n",
    "    -e POSTGRES_PASSWORD=\"root\" \\\n",
    "    -e POSTGRES_DB=\"ny_taxi\" \\\n",
    "    -v c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data \\\n",
    "    -p 5432:5432\n",
    "    postgres:13\n",
    "```\n",
    "\n",
    "- `-e` : pass an environmental variable to the container.\n",
    "- `-v <local path>:<container path>` : map the local filesystem to a volume.\n",
    "- `-p` : map the port from the host machine to the container.\n",
    "\n",
    "**pgcli** is a useful command line tool for working with Postgres databases. Install with Python:\n",
    "```default\n",
    "pip install pgcli\n",
    "```\n",
    "\n",
    "After installation is complete, we can connect to our Postgres container:\n",
    "```default\n",
    "pgcli -h localhost -p 5432 -u root -d ny_taxi\n",
    "```\n",
    "\n",
    "### Connecting pgAdmin\n",
    "pgAdmin is a convenient GUI for interacting with Postgres databases. We can run pgAdmin in its own container:\n",
    "```default\n",
    "docker run -it \\\n",
    "  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n",
    "  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n",
    "  -p 8080:80 \\\n",
    "  dpage/pgadmin4\n",
    "```\n",
    "\n",
    "However, to communicate with our Postgres database running in a separate container, we need to connect the containers using a docker **network**:\n",
    "```default\n",
    "docker network create pg-network\n",
    "```\n",
    "\n",
    "We will need to rerun our previous containers with extra network arguments, so stop the currently running containers:\n",
    "```default\n",
    "docker stop <container id>\n",
    "```\n",
    "\n",
    "Then restart the Postgres and pgAdmin containers with the network commands:\n",
    "```default\n",
    "docker run -it \\\n",
    "    -e POSTGRES_USER=\"root\" \\\n",
    "    -e POSTGRES_PASSWORD=\"root\" \\\n",
    "    -e POSTGRES_DB=\"ny_taxi\" \\\n",
    "    -v c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data \\\n",
    "    -p 5432:5432\n",
    "    --network=pg-network \\\n",
    "    --name pg-database \\\n",
    "    postgres:13\n",
    "\n",
    "docker run -it \\\n",
    "  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n",
    "  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n",
    "  -p 8080:80 \\\n",
    "  --network=pg-network \\\n",
    "  --name pgadmin \\\n",
    "  dpage/pgadmin4\n",
    "```\n",
    "\n",
    "- `--network` : add the container to this network.\n",
    "- `--name` : name this container on the network.\n",
    "\n",
    "Now we can log into pgAdmin and connect to our Postgres database.\n",
    "\n",
    "### Reading data into the Postgres database\n",
    "As an exercise, we will ingest [NY Taxi Trip Record](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) data for January 2021 into the database.\n",
    "\n",
    "Create a folder and download the dataset:\n",
    "```default\n",
    "mkdir ny_taxi_data && cd ny_taxi_data\n",
    "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\n",
    "```\n",
    "\n",
    "Alternatively, you can download the gzipped dataset from: `https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz`\n",
    "\n",
    "We will read the data into the database using the Python libraries `pandas` and `sqlalchemy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "def main(params: argparse.Namespace) -> None:\n",
    "    user = params.user\n",
    "    password = params.password\n",
    "    host = params.host \n",
    "    port = params.port \n",
    "    db = params.db\n",
    "    table_name = params.table_name\n",
    "    url = params.url\n",
    "\n",
    "    csv_name = 'output.csv.gz'\n",
    "\n",
    "    # download the parquet file\n",
    "    os.system(f\"wget {url} -O {csv_name}\")\n",
    "\n",
    "    engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\n",
    "\n",
    "    df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000, compression='gzip')\n",
    "\n",
    "\n",
    "    for df_chunk in df_iter:\n",
    "        t_start = time()\n",
    "\n",
    "        df_chunk.tpep_pickup_datetime = pd.to_datetime(df_chunk.tpep_pickup_datetime)\n",
    "        df_chunk.tpep_dropoff_datetime = pd.to_datetime(df_chunk.tpep_dropoff_datetime)\n",
    "\n",
    "        df_chunk.to_sql(name=table_name, con=engine, if_exists='append')\n",
    "\n",
    "        t_end = time()\n",
    "        print(f'inserted chunk in {t_end - t_start:.3f} seconds.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parse the command line arguments and calls the main program\n",
    "    parser = argparse.ArgumentParser(description='Ingest CSV data to Postgres')\n",
    "\n",
    "    parser.add_argument('--user', help='user name for postgres')\n",
    "    parser.add_argument('--password', help='password for postgres')\n",
    "    parser.add_argument('--host', help='host for postgres')\n",
    "    parser.add_argument('--port', help='port for postgres')\n",
    "    parser.add_argument('--db', help='database name for postgres')\n",
    "    parser.add_argument('--table_name', help='name of the table where we will write the results to')\n",
    "    parser.add_argument('--url', help='url of the csv file')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Compose\n",
    "Managing several separate containers can quickly become cumbersome. Docker Compose is a tool for configuring and running multi-container docker applications within a *single* YAML file.\n",
    "\n",
    "In the YAML file, we define **services**, **networks**, and **volumes**.\n",
    "\n",
    "#### Services\n",
    "Services define the separate containers and their configurations. We first name the service (ex. `pgdatabase`) and then we can either pull an image from Docker Hub or build an image from a Dockerfile.\n",
    "\n",
    "```yaml\n",
    "services:\n",
    "    pgdatabase:\n",
    "        image: postgres:13  # pull an image \n",
    "        ...\n",
    "    custom-container:\n",
    "        build: /path/to/dockfile/  # build an image from a file\n",
    "        image: custom-container  # name the image\n",
    "        ...\n",
    "    custom-online-container:\n",
    "        build: https://github.com/custom/online/container.git  # build an image from a url\n",
    "        image: custom-online-container  # name the image\n",
    "```\n",
    "\n",
    "#### Networks\n",
    "By default a network is created for all services defined in a docker compose file. A service can communicate with another service on the same network by referencing the container name and port.\n",
    "\n",
    "Ports are usually exposed within the images but to communicate with containers from the host machine, ports must be mapped in the compose file:\n",
    "\n",
    "```yaml\n",
    "services:\n",
    "    pgdatabase:\n",
    "        image: postgres:13\n",
    "        ports:\n",
    "            -\"5432:5432\"\n",
    "```\n",
    "\n",
    "We can now communicate with the `pgdatabase` container through port `5432`.\n",
    "\n",
    "We can also define additional virtual networks to segregate our containers if needed:\n",
    "\n",
    "```yaml\n",
    "services:\n",
    "    pgdatabase:\n",
    "        image: postgres:13\n",
    "        networks:\n",
    "            - database-network\n",
    "    otherservice:\n",
    "        image: python:3.11\n",
    "        networks:\n",
    "            - python-network\n",
    "\n",
    "networks:\n",
    "    database-network: {}\n",
    "    python-network: {}\n",
    "```\n",
    "\n",
    "#### Volumes\n",
    "There are three types of volumes: *anonymous*, *named*, and *host*.\n",
    "\n",
    "Docker manages anonymous and named volumes, automatically mounting them in self-generated directories in the host. Host volumes allow us to specify an existing folder in the host.\n",
    "\n",
    "We can configure host volumes at the service level and named volumes at the top level to make named volumes visible to other containers.\n",
    "\n",
    "```yaml\n",
    "services:\n",
    "    pgdatabase:\n",
    "        image: postgres:13\n",
    "        volumes:\n",
    "            - /named-global-volume:/volumes/global-volume\n",
    "            - /home:/volumes/read-write-volume:rw  # rw indicates read/write permissions\n",
    "            - /home:/volumes/read-only-volume:ro  # ro indicates read only\n",
    "        ...\n",
    "    pgadmin:\n",
    "        image: dpage/pgadmin4\n",
    "        volumes:\n",
    "            - /named-global-volume:/volumes/another-volume\n",
    "        ...\n",
    "volumes:\n",
    "    named-global-volume:        \n",
    "```\n",
    "\n",
    "In this case, both containers will have read/write access to the `named-global-volume` shared folder, regardless of the path they've mapped it to in the container.\n",
    "\n",
    "#### Dependencies\n",
    "Often we need to create a dependency chain between services so that they run in a certain order (like starting up a Postgres database before pdAdmin). We can also specify conditions and requirements to control what happens when dependent services start or complete.\n",
    "\n",
    "```yaml\n",
    "services:\n",
    "    pgdatabase:\n",
    "        image: postgres:13\n",
    "        depends_on:\n",
    "            pgadmin:\n",
    "                condition: service_healthy  # check if service is \"healthy\" before starting \n",
    "        ...\n",
    "    pgadmin:\n",
    "        image: dpage/pgadmin4\n",
    "        ...\n",
    "    \n",
    "```\n",
    "\n",
    "```yaml\n",
    "services:\n",
    "    pgdatabase:\n",
    "        image: postgres:13\n",
    "        enviroment:\n",
    "            - POSTGRES_USER=root\n",
    "            - POSTGRES_PASSWORD=root\n",
    "            - POSTGRES_DB=ny_taxi\n",
    "        volumes:\n",
    "            - \"./ny_taxi_data:/var/lib/postgresql/data:rw\"\n",
    "        ports:\n",
    "            - \"5432:5432\"\n",
    "    pgadmin:\n",
    "        image: dpage/pgadmin4\n",
    "        environment:\n",
    "            - PGADMIN_DEFAULT_EMAIL=admin@admin.com\n",
    "            - PGADMIN_DEFAULT_PASSWORD=root\n",
    "        ports:\n",
    "            - \"8080:80\"\n",
    "```\n",
    "\n",
    "#### Environment Variables\n",
    "Working with environment variables is easy in Compose. We can define static variables or we can use dynamic environment variables enclosed with `${}` and specified with a `.env` file:\n",
    "```yaml\n",
    "services:\n",
    "    pgdatabase:\n",
    "        image: postgres:${POSTGRES_VERSION}\n",
    "        environment:\n",
    "            DB: ${DATABASE_NAME}\n",
    "        env_file:\n",
    "            - .env\n",
    "```\n",
    "\n",
    "Where the `.env` file contains:\n",
    "```default\n",
    "# .env\n",
    "POSTGRES_VERSION=13\n",
    "DATABASE_NAME=mydb\n",
    "```\n",
    "\n",
    "Then we can run these containers with:\n",
    "```default\n",
    "docker compose -f compose.yml up -d --build\n",
    "```\n",
    "\n",
    "And to shutdown these containers, we can use:\n",
    "```default\n",
    "docker compose down -v\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
