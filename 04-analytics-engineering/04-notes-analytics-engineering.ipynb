{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Analytics Engineering?\n",
    "- The **analytics** engineer role sites between a data engineer, who prepares and maintains the data infrastructure and the data analyst, who is using data to answer business questions and solve problems.\n",
    "- The analytics engineer introduces good software engineering practices to the efforts of data analysts and data engineers.\n",
    "\n",
    "Remember the tradition data process looks something like this:\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Data loading] --> B[Data storing]\n",
    "    B[Data storing] --> C[Data modeling]\n",
    "    C[Data modeling] --> D[Data presentation]\n",
    "```\n",
    "\n",
    "Analytics engineers are often focused on the data modeling and data presentation steps. But what does it mean to \"model\" data? Today there are several popular approaches to modeling data and building data warehouses.\n",
    "\n",
    "1. Kimball Dimensional Modeling (Star Schema)\n",
    "- Ralph Kimball is a prominent figure in the field of data warehousing and business intelligence. He pioneered dimensional modeling and wrote the book \"The Data Warehouse Toolkit\", an essential reference for creating and maintaining data warehouses.\n",
    "- Kimball focuses on building simple, practical, and accessible data warehouses, optimized for query performance and ease of use by end users. It employs a bottom-up approach, where smaller, subject-specific data marts are built first and then integrated into a larger enterprise data warehouse.\n",
    "- The data model relies on a **star schema** design, where data is organized into facts (measureable, quantitative data) and related dimensions (descriptive attributes).\n",
    "- Kimball allows data denormalization where needed, which can lead to data redundancy but enables simplified querying and improved performance.\n",
    "- It emphasizes iterative development and delivering business value in smaller increments.\n",
    "\n",
    "2. Inmon Methodology\n",
    "- Bill Inmon is another prominent figure in the field of data warehousing and business intelligence, often called the \"father of data warehousing\". He coined the term \"data warehouse\" and wrote several influential books, including \"Building the Data Warehouse\".\n",
    "- Inmon prioritizes data integration and consistency across an organization. It employes a top-down approach, where the enterprise data warehouse is built first as a centralized repository for all organizational data and then data marts are created from this source as needed.\n",
    "- Inmon emphasizes normalizing data in third normal form (3NF) to reduce data redundancy and ensure data integrity.\n",
    "- This approach allows data warehouses and data models to be more scalable and adaptable to changing business needs due to its centralized nature.\n",
    "- However this approach is usually more complex and time-consuming to implement when compared to Kimball's approach.\n",
    "\n",
    "3. Data Vault\n",
    "- Data Vault was created by Dan Linstedt in the late 1990s as a reaction to real world challenges he encountered while working on large data warehouse projects. His book \"Building a Scalable Data Warehouse with Data Vault 2.0\" describes the data vault approach.\n",
    "- A data vault is set up in a hub-and-spoke architecture, with three core components: hubs (containing business keys), links (joining keys between hubs), and satellites (containing descriptive attributes).\n",
    "- This approach excels in capturing historical data changes and providing a detailed audit trail for data.\n",
    "- Its highly adaptable to changing business needs and is highly scalable due to its modular design.\n",
    "- However data vault modeling can be complex to understand and implement, especially for business users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of dimensional modeling\n",
    "In this course we will use the dimensional modeling approach. Dimensional modeling includes fact tables and dimension tables.\n",
    "\n",
    "* Fact tables\n",
    "    - Record measurements, metrics, or facts that correspond to a business process. Ex. \"verbs\" like sales, orders, transactions.\n",
    " * Dimension tables\n",
    "    - Corresponds to business entities that provide context to a business process. Ex. \"nouns\" like customer, product, regions.\n",
    "\n",
    "The architecture containing a dimensional model is usually composed of a staging area, a processing area, and a presentation area:\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    A[\"Stage Area (raw data)\"] --> B[\"Processing Area (data models)\"]\n",
    "    B[\"Processing Area (data models)\"] --> C[\"Presentation Area (reports, dashboards)\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data modeling with dbt\n",
    "**dbt** (**d**ata **b**uild **t**ool) is a transformation workflow tool that modularizes SQL code into discrete units called **models**. These models represent individual transformations or business logic applied to the data.\n",
    "\n",
    "Models are written using SQL within Jinja templates and are then compiled into `*.sql` files.\n",
    "\n",
    "dbt provides several other tools for:\n",
    "- Dependency management: users can define dependencies between models to ensure they are executed in the correct order\n",
    "- Version control: dbt integrates with version control systems like git, allowing data transformations and business logic to be tracked across time.\n",
    "- Testing: it includes a testing framework to enable users to test their data models.\n",
    "\n",
    "dbt usually sits on top of a data warehouse, processing data as its ingested as well as throughout the warehouse.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    c[Data loaders] --> a\n",
    "    subgraph s1 [dbt]\n",
    "        subgraph ss1 [Data warehouse]\n",
    "            a[Raw data] --> b[Transformed data]\n",
    "        end\n",
    "    end\n",
    "    b --> d[BI Tools]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to use dbt:\n",
    "1. dbt Core\n",
    "    - dbt Core is an open-source command line tool used to set up, build, and run dbt projects (which are made up of `.sql` and `.yml` files).\n",
    "    - --> [Installation instructions](https://docs.getdbt.com/docs/core/installation-overview)\n",
    "\n",
    "2. dbt Cloud\n",
    "    - [dbt Cloud](https://cloud.getdbt.com/) is a web-based IDE application used to develop, test, and run dbt projects.\n",
    "    - Make sure to setup a Project subdirectory in your connected Repository under `Account settings` > `Project` > `<project name>` > `Project subdirectory`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing dbt models\n",
    "dbt uses a modular data modeling approach similar to dimensional modeling.\n",
    "\n",
    "```mermaid\n",
    "\n",
    "```\n",
    "\n",
    "dbt models are essentially Jinja templated SQL scripts saved as `.sql` where dbt specific functions take care of the DDL (data definition) and we write the DML (data manipulation):\n",
    "\n",
    "```sql Example dbt model\n",
    "{{\n",
    "    config(materialized='table')\n",
    "}}\n",
    "\n",
    "select *\n",
    "from staging.source_table\n",
    "where record_state = 'ACTIVE'\n",
    "```\n",
    "\n",
    "This script is then compiled by dbt into the following sql code:\n",
    "\n",
    "```sql\n",
    "create table my_schema.my_model as (\n",
    "    select *\n",
    "    from staging.source_table\n",
    "    where record_state = 'ACTIVE'\n",
    ")\n",
    "```\n",
    "\n",
    "Then running the `dbt run` command will pick up all of the *templated* `.sql` files and compile them into executable sql scripts.\n",
    "\n",
    "There are 4 ways to create or *materialize* tables in dbt:\n",
    "1. Ephemeral: tables are temporary and exist only for the duration of a single dbt run.\n",
    "2. View: virtual tables created by dbt that can be queried like a regular table.\n",
    "3. Table: physical representation of data that are created and stored in the database.\n",
    "4. Incremental: physical representation of data but allows for efficient updates to existing tables, reducing the need for full data refreshes.\n",
    "\n",
    "Example data model:\n",
    "```mermaid\n",
    "    flowchart LR\n",
    "        A[green_tripdata] --> B\n",
    "        B[\"green_tripdata cleaned\"] --> C[fact_trips]\n",
    "        D[yellow_tripdata] --> E\n",
    "        E[\"yellow_tripdata cleaned\"] --> C[fact_trips]\n",
    "        F[taxi_zone_lookup] --> G\n",
    "        C[fact_trips] --> H[a]\n",
    "        G[a] --> H[a]\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The FROM clause of a dbt model\n",
    "We have two ways of selecting from data sources in dbt, `sources` and `seeds`.\n",
    "\n",
    "For `sources`, we first define the data sources for dbt in a `sources.yaml` file under the `sources` block. We can then use the `source` function to call the correct data source and schema:\n",
    "```sql\n",
    "...\n",
    "from {{ source('<schema>', '<data source name>') }}\n",
    "```\n",
    "We can also use this to run tests against our source data (such as testing for fresh data).\n",
    "\n",
    "For `seeds`, we can query csv files stored in our repository under the `seeds` folder by using the `ref` function.\n",
    "```sql\n",
    "...\n",
    "from {{ ref('<model name>') }}\n",
    "```\n",
    "This also means that the source file is version controlled. This is usually used for data that does not change frequently.\n",
    "\n",
    "The `ref` function allows us to reference the underlying tables and views built in the data warehouse. It allows us to run the same code in any environment, it will resolve the correct schema and automatically built the dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macros\n",
    "**Macros** are special Jinja template markup blocks used by dbt to generate SQL code. This allows us to use control structures and  functions in our SQL code and can make our SQL more reusable and portable.\n",
    "\n",
    "Quick template markup overview:\n",
    "- `{##}` marks comments.\n",
    "- `{%%}` marks code blocks for control flow or calling functions.\n",
    "- `{{}}` marks variables.\n",
    "\n",
    "\n",
    "## Packages\n",
    "Similar to code libraries or modules, dbt **packages** provide predefined macros that can be used in a project. By adding a package to your project, the package's models and macros will become part of your project.\n",
    "\n",
    "They are imported in the `package.yml` files and import by running `dbt deps`.\n",
    "\n",
    "A list of useful packages can be found at [dbt Package Hub](hub.getdbt.com). A couple of recommended packages:\n",
    "* `dbt_utils` (see Github)\n",
    "* `codegen` (see Github)\n",
    "* `dbt_expectations` (see Github)\n",
    "\n",
    "## Variables\n",
    "Variables define values used accross the project. To use a variable we reference is as `{{ var('...') }}`. Variables can be defined in two ways:\n",
    "* In the `dbt_project.yml` file.\n",
    "* On the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and documenting dbt models\n",
    "### Testing\n",
    "dbt tests are assumptions made about our data using a `SELECT` statement. These assumptions are compiled to sql that returns the amount of failing records.\n",
    "* Tests are defined per column in the `schema.yml` file.\n",
    "* dbt provides basic tests to check if the column values are unique, not null, accepted values, or a foreign key to another table.\n",
    "* We can create custom tests as queries.\n",
    "\n",
    "### Documentation\n",
    "dbt provides a way to generate documentation for your project and render it as a website.\n",
    "The documentation for your project includes:\n",
    "* Project information\n",
    "    - Model code (both from the .sql file and compiled)\n",
    "    - Model dependencies\n",
    "    - Source\n",
    "    - Auto generate DAG from the ref and source macros\n",
    "    - Descriptions (from .yml file) and test\n",
    "* Data Warehouse Information\n",
    "    - Column names and data types\n",
    "    - Table stats like size and rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
