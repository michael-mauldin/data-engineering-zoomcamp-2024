{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Workflow Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: DTC DE Zoomcamp is using Mage as its orchestrator for the 2024 class cohort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is workflow orchestration?\n",
    "A large part of data engineering is **extracting**, **transforming**, and **loading** data between sources.\n",
    "\n",
    "```mermaid\n",
    "---\n",
    "title: Data engineering lifecycle\n",
    "---\n",
    "flowchart LR\n",
    "    A[Data generation] --> B\n",
    "    subgraph \"Storage\"\n",
    "    B[Ingestion] --> C[Tranformation] --> D[Serving]\n",
    "    end\n",
    "    D --> E(Analytics)\n",
    "    D --> F(Machine Learning)\n",
    "    D --> G(Reverse ETL)\n",
    "```\n",
    "\n",
    "**Orchestration** is the process of dependency management between these sources, facilitated through automation.\n",
    "\n",
    "The data **orchestrator** manages scheduling, triggering, monitoring, and resource allocation.\n",
    "- Every **workflow** requires *sequential* steps or tasks.\n",
    "- Workflows are typically represented as DAGS (directed acyclic graphs).\n",
    "\n",
    "A good orchestrator handles workflow management, automation, error handling and recovery, monitoring and alerting, resource optimization, observability and debugging, and compliance and auditing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Mage\n",
    "Mage is an open-source pipeline tool for orchestrating, transforming, and ingregrating data.\n",
    "\n",
    "```mermaid\n",
    "---\n",
    "title: Mage Workflow\n",
    "---\n",
    "flowchart LR\n",
    "    A[Projects] --> B[Pipelines] --> C[Blocks]\n",
    "    C --> D[Load]\n",
    "    C --> E[Transform]\n",
    "    C --> F[Export]\n",
    "```\n",
    "\n",
    "- A **project** contains the code for all pipelines, blocks, and other assets (the \"repo\" of your orchestrator).\n",
    "\n",
    "- A **pipeline** is a workflow that executes some data operation (extracting, transforming, loading), also called a DAG. Each pipeline is represented by a YAML file in the \"pipelines\" fodler of your project.\n",
    "\n",
    "- A **block** is a chunk of code (SQL, Python, or R) that can be executed independently or within a pipeline. Together, blocks form DAGs called pipelines. A block won't start running in a pipeline until all its upstream dependencies are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring Mage\n",
    "1. `git clone` the repo at https://github.com/mage-ai/mage-zoomcamp\n",
    "2. Pull the latest Mage image with `docker pull mageai/mageai:latest`.\n",
    "3. Run `docker compose build` to set up the environment.\n",
    "4. Run `docker compose up` to run the environment.\n",
    "5. Navigate to [localhost:6789](http://localhost:6789) to enter the Mage environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Postgres to work with Mage\n",
    "\n",
    "```yaml\n",
    "postgres:\n",
    "    image: postgres:15\n",
    "    restart: on-failure\n",
    "    container_name: ${PROJECT_NAME}-postgres\n",
    "    env_file:\n",
    "        - .env\n",
    "    environment:\n",
    "        POSTGRES_DB: ${POSTGRES_DBNAME}\n",
    "        POSTGRES_USER: ${POSTGRES_USER}\n",
    "        POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n",
    "    port:\n",
    "        - \"${POSTGRES_PORT}:5432\"\n",
    "```\n",
    "\n",
    "In Mage, connections are managed in `io_config.yml`. Connections can have different profiles, the default profile is `default`. To add another profile:\n",
    "```yaml\n",
    "dev:\n",
    "    POSTGRES_CONNECT_TIMEOUT: 10\n",
    "    POSTGRES_DBNAME: \"{{ env_var('POSTGRES_DBNAME') }}\"\n",
    "    POSTGRES_SCHEMA: \"{{ env_var('POSTGRES_SCHEMA') }}\"\n",
    "    POSTGRES_USER: \"{{ env_var('POSTGRES_USER') }}\"\n",
    "    POSTGRES_PASSWORD: \"{{ env_var('POSTGRES_PASSWORD') }}\"\n",
    "    POSTGRES_HOST: \"{{ env_var('POSTGRES_HOST') }}\"\n",
    "    POSTGRES_PORT: \"{{ env_var('POSTGRES_PORT') }}\"\n",
    "```\n",
    "\n",
    "Note that injecting variables into `io_config.yml` is done with Jinja template tags.\n",
    "\n",
    "Next test the Postgres connection by creating a SQL block, set it to use Postgres SQL, the `dev` profile, and check `Use raw SQL`. Then run a simple query `SELECT 1` against the database to check that it's connecting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from API to Posgres\n",
    "\n",
    "#### Loading the data \n",
    "1. Create a new batch pipeline named `api_to_postgres`.\n",
    "2. Create a Data Loader block with an API Template named `load_api_data`.\n",
    "3. Update the block with:\n",
    "```Python\n",
    "@data_loader\n",
    "def load_data_from_api(*args, **kwargs):\n",
    "    url = \"\"\n",
    "    # declare data types ahead of import\n",
    "    taxi_dtypes = {\n",
    "        'VendorID': pd.Int64Dtype(),\n",
    "        'passenger_count': pd.Int64Dtype(),\n",
    "        'trip_distance': float,\n",
    "        'RatecodeID':pd.Int64Dtype(),\n",
    "        'store_and_fwd_flag':str,\n",
    "        'PULocationID':pd.Int64Dtype(),\n",
    "        'DOLocationID':pd.Int64Dtype(),\n",
    "        'payment_type': pd.Int64Dtype(),\n",
    "        'fare_amount': float,\n",
    "        'extra':float,\n",
    "        'mta_tax':float,\n",
    "        'tip_amount':float,\n",
    "        'tolls_amount':float,\n",
    "        'improvement_surcharge':float,\n",
    "        'total_amount':float,\n",
    "        'congestion_surcharge':float\n",
    "    }\n",
    "\n",
    "    parse_dates =['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "\n",
    "    return pd.read_csv(url, sep=',', compression='gzip', dtype=taxi_dtypes, parse_dates=date_columns)\n",
    "```\n",
    "4. Run the block.\n",
    "\n",
    "#### Transform the data\n",
    "1. Create a Transformer block named `transform_taxi_data`.\n",
    "2. Update the block with:\n",
    "```Python\n",
    "def transform(data, *args, **kwargs):\n",
    "    print(f\"Preprocessing: rows with zero passengers: {data['passenger_count'].isin([0]).sum()}\")\n",
    "\n",
    "    return data[data['passenger_count'] > 0]\n",
    "\n",
    "@test\n",
    "def test_output(output, *args, **kwargs):\n",
    "    assert output['passenger_count'].isin([0]).sum() == 0, 'There are rides with zero passenger.'\n",
    "```\n",
    "\n",
    "#### Export the data\n",
    "1. Create a Data Exporter block using a Postgres template named `taxi_data_to_postgres`.\n",
    "2. Update the code block:\n",
    "```Python\n",
    "@data_exporter\n",
    "def export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n",
    "    schema_name = 'ny_taxi'\n",
    "    table_name = 'yellow_cab_data'\n",
    "    config_path = path.join(get_repo_path(), 'io_config.yaml')\n",
    "    config_profile = 'dev'\n",
    "\n",
    "    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n",
    "        loader.export(\n",
    "            df, \n",
    "            schema_name,\n",
    "            table_name,\n",
    "            index=False\n",
    "            if_exists='replace'\n",
    "        )\n",
    "```\n",
    "3. Run the block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring GCP in Mage\n",
    "Set up storage in Google Cloud:\n",
    "1. Create a new Cloud Storage bucket `<unique bucker name>` and check \"Enforce public access prevention on this bucket\" to make your bucket private.\n",
    "2. Create a GCP service account for Mage and create a new key and copy into the local project directory.\n",
    "3. Then within Mage, in `io_config.yaml`, set `GOOGLE_SERVICE_ACC_KEY_FILEPATH=<credentials .json key file>`\n",
    "4. Test the connection by creating a Data Loader block and executing SQL `SELECT 1`.\n",
    "\n",
    "## Loading data from API to GCP\n",
    "1. Create a new batch pipeline and add the `load_api_data` block and `transform_taxi_data` block from the previous steps.\n",
    "2. Create a Data Export with Google Cloud Storage template called `taxi_to_gcs_parquet`.\n",
    "3. Update the block with:\n",
    "```Python\n",
    "@data_exporter\n",
    "def export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:\n",
    "    config_path = path.join(get_repo_path(), 'io_config.yaml')\n",
    "    config_profile = 'default'\n",
    "\n",
    "    bucket_name = '<gcs bucket name>'\n",
    "    object_key = '<gcs bucket key>'\n",
    "\n",
    "    GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).export(\n",
    "        df,\n",
    "        bucket_name,\n",
    "        object_key,\n",
    "    )\n",
    "```\n",
    "4. Run the block and all upstream blocks.\n",
    "\n",
    "This will write the data to one large parquet file. If the data is large enough, it's advised to split the data into **partitioned** parquet files. Datasets are often paritioned by data dimensions or time periods.\n",
    "\n",
    "To write to partitioned parquet files, create a `Data Exporter` block attached to the `Transformer` block with a generic template named `taxi_to_gcs_partitioned` and update the block with:\n",
    "```Python\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "\n",
    "if 'data_exporter' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import data_exporter\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '<path to creds.json>'\n",
    "project_id = '<gcs project id>'\n",
    "bucket_name = '<gcs bucket name>'\n",
    "object_key = 'ny_taxi_data.parquet'\n",
    "table_name = 'ny_taxi_data'\n",
    "root_path = f'{bucket_name}/{table_name}'\n",
    "\n",
    "\n",
    "@data_exporter\n",
    "def export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:\n",
    "    # create a date column from tpep_pickup_datetime column\n",
    "    df['tpep_pickup_date'] = df['tpep_pickup_datetime'].dt.date\n",
    "\n",
    "    table = pa.Table.from_pandas(df)\n",
    "\n",
    "    gcs = pa.fs.GcsFileSystem()\n",
    "\n",
    "    pq.write_to_dataset(\n",
    "        table,\n",
    "        root_path=root_path,\n",
    "        parition_cols=['tpep_pickup_date'],\n",
    "        filesystem=gcs\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from GCS to BigQuery\n",
    "The next step is to take the data stored in parquet files and write it to a OLAP database for further analytics.\n",
    "\n",
    "1. Create a batch pipeline named `gcs_to_bigquery`.\n",
    "2. Create a `Data Loader` block named `load_taxi_gcs` and fill in the `bucket_name` and `object_id`. Run the block to load the data.\n",
    "3. Create a `Transformer` block named `transform_staged_data` where we will standardize the column names with:\n",
    "```Python\n",
    "@transformer\n",
    "def transform(data, *args, **kwargs):\n",
    "    data.columns = data.columns.str.replace(' ', '_').str.lower()\n",
    "\n",
    "    return data\n",
    "```\n",
    "4. Create a `Data Exporter` block with the SQL template named `write_taxi_to_bigquery`. Use a BigQuery connection with the `default` profile and set the schema to `ny_taxi` and the table to `yellow_cab_data`. Then to export data using SQL in Mage, we just have to select the dataframe from the previous step:\n",
    "```SQL\n",
    "SELECT * FROM {{ df_1 }}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling in Mage\n",
    "In Mage, scheduling when and how often we run our pipelines are done with **Triggers**. Triggers can be run according to a **Schedule**, a specific **Event**, or via an  **API**.\n",
    "\n",
    "For this example, we will use a Schedule type Trigger. Click `Schedule` to open a new Trigger; name the trigger `gcs_to_bigquery_schedule`, set the frequency to `daily`, and set a start date. A Mage Schedule Trigger also allows the use of more complex schedules with a [Cron expression](https://docs.oracle.com/cd/E12058_01/doc/doc.1014/e12030/cron_expressions.htm). \n",
    "\n",
    "Save the changes to the Trigger and click `Enable trigger`. Now the Trigger is active and ready to run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterized Execution\n",
    "We can pass variables or parameters through entire DAGs when loading data in Mage in a couple different ways. The first option is to pass variables between blocks via `**kwargs**`. For example:\n",
    "\n",
    "```Python\n",
    "def export_taxi_data_to_gcp_parameter(df: DataFrame, **kwargs) -> None:\n",
    "\n",
    "    # we can access variables passed through **kwargs by\n",
    "    now = kwargs.get('execution_date')\n",
    "```\n",
    "\n",
    "Another options is set pipeline global variables through the pipeline editor itself or runtime variables in the Triggers page of the GUI.\n",
    "\n",
    "## Backfills\n",
    "**Backfills** allow us to run a pipeline multiple times with different parameters. This is usually used to populate historical or lost data especially when pipelines depend on date and time windows. For example, if a pipeline that collects daily timestamped data goes down for a couple of days, we can use backfills to rerun the pipeline for those outage days to *backfill* the missing data.\n",
    "\n",
    "Navigate to the Backfills tab on the left navigation bar in the Mage GUI and `Create a new backfill`.\n",
    "\n",
    "From here, we can configure the Backfill to run our pipeline a specific number of times based on a start date, an end date, and an interval.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
