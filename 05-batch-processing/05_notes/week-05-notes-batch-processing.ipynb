{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch processing data\n",
    "Batch processing refers to partitioning and processing data in blocks called **batches**. Data is usually partitioned by time intervals (weekly, daily, hourly, 5 min, etc.).\n",
    "\n",
    "The advantages of batch processing include:\n",
    "* Easy to manage pipelines.\n",
    "* Easy to retry or repair pipeline workflows.\n",
    "* East to scale and parallelize.\n",
    "\n",
    "Some disadvantages include:\n",
    "* Slower execution times (in many cases need to rerun entire pipeline).\n",
    "\n",
    "Batch processing is the most common type of data processing in industry.\n",
    "\n",
    "\n",
    "## Apache Spark\n",
    "The most common tool used today for batch processing is Apache Spark. Spark is a multi-language open-source analytics **engine** for large-scale data processing that provides an interface for programming distributed machine clusters. Spark's main language is Scala, but it also has wrappers for Java, Python, etc. PySpark is usually the preferred wrapper for Spark.\n",
    "\n",
    "Spark is usually used to process files from a data lake and saved back into the data lake.\n",
    "\n",
    "```mermaid\n",
    "    flowchart LR\n",
    "        A(Raw Data) --> B\n",
    "        B[(Data Lake)] --> C\n",
    "        C[\"SQL (Athena or Presto)\"] --> D\n",
    "        D[Spark] --> B\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to PySpark\n",
    "Spark works over distributed coordinated clusters. There are two main abstractions:\n",
    "* RDD (): a distributed collection of objects.\n",
    "* Dataframe: a distributed dataset of tabular data.\n",
    "\n",
    "There are two important concepts in PySpark are:\n",
    "* Immutability: changes create new object references, old version are unchanged.\n",
    "* Actions vs. Transformations: Spark commands are either transformations or actions.\n",
    "    - Transformations are **lazy**, meaning the actual compute does not happen until an output is requested. This allows Spark to collect all actions and make optimizations when the output is requested.\n",
    "    - Actions are **eager**, meaning that they are evaulated immediately. This applies to commands such as `show`, `take`, `head`, etc.\n",
    "\n",
    "#### Simplified Spark Architectural Overview\n",
    "The user interacts with the **driver** which controls **executors** run on a **master** that operates on the data.\n",
    "```mermaid\n",
    "    flowchart LR\n",
    "        A[Driver] --> B\n",
    "        subgraph \"Master\"\n",
    "            B[Executors]\n",
    "        end\n",
    "        B --> C[Data]\n",
    "```\n",
    "\n",
    "#### Common operations\n",
    "Load a csv\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .options(header=True, inferSchema=True) \\\n",
    "    .csv(\"mtcars.csv\")\n",
    "```\n",
    "\n",
    "View a dataframe\n",
    "```python\n",
    "df.show()\n",
    "df.show(10)  # specific number of rows\n",
    "```\n",
    "\n",
    "View columns and datatypes\n",
    "```python\n",
    "df.columns\n",
    "df.dtypes\n",
    "```\n",
    "\n",
    "Rename columns\n",
    "```python\n",
    "df.toDF('a', 'b', 'c')\n",
    "df.withColumnRenamed('old', 'new')  # rename specific column\n",
    "```\n",
    "\n",
    "Drop columns\n",
    "```python\n",
    "df.drop('mpg')\n",
    "```\n",
    "\n",
    "Filtering\n",
    "```python\n",
    "df[df.mpg < 20]\n",
    "df[(df.mpg < 20) & (df.cyl == 6)]\n",
    "```\n",
    "\n",
    "Add columns\n",
    "```python\n",
    "df.withColumn('gpm', 1 / df.mpg)\n",
    "```\n",
    "\n",
    "Fill nulls\n",
    "```python\n",
    "df.fillna(0)\n",
    "```\n",
    "\n",
    "Aggregation\n",
    "```python\n",
    "df.groupby(['cyl', 'gear']) \\\n",
    "    .agg({'mpg': 'mean', 'disp': 'min'})\n",
    "```\n",
    "\n",
    "Standard Transformations\n",
    "There are tons of common transformations available in the `functions` module.\n",
    "```python\n",
    "import pyspark.sql.functions as F\n",
    "df.withColumn('logdisp', F.log(df.disp))\n",
    "```\n",
    "Using transformations from the `functions` module keeps the code execution within the JVM and keeps the execution performant.\n",
    "\n",
    "Row conditional statements\n",
    "```python\n",
    "import pyspark.sql.functions as F\n",
    "df.withColumn('cond', \\\n",
    "    F.when(df.mpg > 20, 1) \\\n",
    "    .when(df.cyl == 6, 2) \\\n",
    "    .otherwise(3)\n",
    ")\n",
    "```\n",
    "\n",
    "When Python is required\n",
    "You can register UDFs (User Defined Functions).\n",
    "```python\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "fn = F.udf(lambda x: x+1, DoubleType())\n",
    "df.withColumn('disp1', fn(df.disp))\n",
    "```\n",
    "It is important that the UDF be deterministic, because Spark may evaulate the function more than once or apply optimizations.\n",
    "\n",
    "Merge/join dataframes\n",
    "```python\n",
    "left.join(right, on='key')\n",
    "left.join(right, left.a == right.b)\n",
    "```\n",
    "\n",
    "Pivot tables\n",
    "```python\n",
    "df.groupBy('A', 'B').pivot('C').sum('D')\n",
    "```\n",
    "\n",
    "Summary statistics\n",
    "```python\n",
    "df.describe().show()  # display count, mean, stddev, min, max\n",
    "# percentiles:\n",
    "df.selectExpr(\n",
    "    \"percentile_approx(mpg, array(.25, .5, .75)) as mpg\"\n",
    ").show()\n",
    "```\n",
    "\n",
    "Plotting\n",
    "There is not an options for plotting directly in PySpark, but you can export to Pandas for plotting. This is not advised.\n",
    "```python\n",
    "df.sample(False, 0.1).toPandas().hist()\n",
    "```\n",
    "\n",
    "SQL\n",
    "Spark allows you to switch between SQL and Dataframes. To use our dataframe within the SQL query, we need to register the dataframe as view or table:\n",
    "```python\n",
    "df.createOrReplaceTempView('foo')  # registering a table in SQL\n",
    "```\n",
    "\n",
    "Then we can use SQL to reference our dataframe like a table and query it:\n",
    "```python\n",
    "df2 = spark.sql('select * from foo')\n",
    "```\n",
    "\n",
    "#### PySpark Best Practices\n",
    "* Use `pyspark.sql.functions` and other built in functions.\n",
    "* Use the same version of python and packages on cluster as the driver.\n",
    "* Check the UI at http://localhost:4040/.\n",
    "* Learn about SSH port forwarding for working with Spark in a notebook.\n",
    "* Check out Spark MLlib, basically the Spark equivalent of scikit-learn.\n",
    "* Check out the docs at https://spark.apache.org/docs/latest\n",
    "\n",
    "#### Things to avoid\n",
    "* Iterating through rows.\n",
    "* Hard code a master in your driver (use command `spark-submit` for that).\n",
    "* Filter before conversion to Pandas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing and running Spark\n",
    "\n",
    "#### Installing Java\n",
    "Download OpenJDK 11 or Oracle JDK 11 (It's important that the version is 11 - spark requires 8 or 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz\n",
    "!tar xzfv openjdk-11.0.2_linux-x64_bin.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export JAVA_HOME=\"${HOME}/spark/jdk-11.0.2\"\n",
    "!export PATH=\"${JAVA_HOME}/bin:${PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm openjdk-11.0.2_linux-x64_bin.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Spark\n",
    "Download Spark 3.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xzfv spark-3.3.2-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm spark-3.3.2-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export SPARK_HOME=\"${HOME}/spark/spark-3.3.2-bin-hadoop3\"\n",
    "!export PATH=\"${SPARK_HOME}/bin:${PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add environmental variables to `.bashrc` file to save: `nano .bashrc` and copy in the variable definitions:\n",
    "```\n",
    "export JAVA_HOME=\"${HOME}/spark/jdk-11.0.2\"\n",
    "export PATH=\"${JAVA_HOME}/bin:${PATH}\"\n",
    "\n",
    "export SPARK_HOME=\"${HOME}/spark/spark-3.3.2-bin-hadoop3\"\n",
    "export PATH=\"${SPARK_HOME}/bin:${PATH}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add PySpark to `PYTHONPATH`:\n",
    "!export PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\n",
    "!export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively use [findspark](https://github.com/minrk/findspark) to add PySpark to sys.path at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that the installation worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Spark session with a local cluster using all available cpus.\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.getConf().set('spark.ui.port', '4040')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to Google Cloud Storage in Spark\n",
    "To read data from a GCS datalake we can use the [Google Cloud Storage connector for Hadoop](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters).\n",
    "\n",
    "Download [gcs-connector-hadoop3-latest.jar](https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar) and move the `.jar` file to a `/jars` directory within your Spark directory.\n",
    "\n",
    "Within PySpark, import the following:\n",
    "```python\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "```\n",
    "\n",
    "And configure the Spark context:\n",
    "```python\n",
    "credentials_location = './path/to/google_credentials.json'\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set('spark.jars', './apache-spark/<version>/jars/gcs-connector-hadoop3-latest.jar') \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
    "```\n",
    "\n",
    "Then build the SparkSession with the new parameters:\n",
    "```python\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "This will allow you to  read files directly from GCS:\n",
    "```python\n",
    "df_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
